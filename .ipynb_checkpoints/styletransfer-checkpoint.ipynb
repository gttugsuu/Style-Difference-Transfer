{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from utility.utility import postp, GramMatrix, GramMSELoss, load_images, save_images, make_folders\n",
    "from utility.loss_fns import get_style_patch_weights, patch_difference, mrf_loss_fn, weight_maker\n",
    "from utility.vgg_network import VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# style weights\n",
    "sw1=1\n",
    "sw2=1\n",
    "sw3=1\n",
    "sw4=1\n",
    "sw5=1\n",
    "# Content weights\n",
    "cw1=0\n",
    "cw2=0\n",
    "cw3=0\n",
    "cw4=1e5\n",
    "cw5=0\n",
    "#############################################################################\n",
    "# Get image paths and names\n",
    "# Style 1\n",
    "style_dir1  = os.path.dirname('../input/font_contents/serif/A/Milonga-Regular.png')\n",
    "style_name1 = os.path.basename('../input/font_contents/serif/A/Milonga-Regular.png')\n",
    "# Style 2\n",
    "style_dir2  = os.path.dirname('../input/font_contents/serif_rmv/A/Milonga-Regular.png')\n",
    "style_name2 = os.path.basename('../input/font_contents/serif_rmv/A/Milonga-Regular.png')\n",
    "# Content\n",
    "content_dir  = os.path.dirname('../input/font_contents/AlegreyaSans-Light/A.png')\n",
    "content_name = os.path.basename('../input/font_contents/AlegreyaSans-Light/A.png')\n",
    "\n",
    "# Cuda device\n",
    "if torch.cuda.is_available:\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "# Parameters\n",
    "alpha = 1\n",
    "beta = 1\n",
    "patch_size = 5\n",
    "image_size = 256\n",
    "content_invert = 1\n",
    "style_invert = 1\n",
    "result_invert = content_invert\n",
    "\n",
    "# Get output path\n",
    "output_path = '../output_style_difference/'\n",
    "try:\n",
    "    os.mkdir(output_path)\n",
    "except:\n",
    "    pass\n",
    "output_path = output_path + content_name[:-4] + '_' + style_name1[:-4] + '_' + style_name2[:-4] + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get network\n",
    "vgg = VGG()\n",
    "vgg.load_state_dict(torch.load('../Models/vgg_conv.pth'))\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "if torch.cuda.is_available():\n",
    "    vgg.cuda()\n",
    "\n",
    "# Load images\n",
    "content_image = load_images(os.path.join(content_dir, content_name), image_size, device, content_invert)\n",
    "style_image1  = load_images(os.path.join(style_dir1,style_name1), image_size, device, style_invert)\n",
    "style_image2  = load_images(os.path.join(style_dir2,style_name2), image_size, device, style_invert)\n",
    "\n",
    "# Random input\n",
    "# opt_img = Variable(torch.randn(content_image.size()).type_as(content_image.data).to(device), requires_grad=True).to(device)\n",
    "# Content input\n",
    "opt_img = Variable(content_image.data.clone(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layers, loss functions, weights and compute optimization targets\n",
    "# Style layers\n",
    "# style_layers = ['r11','r21','r31','r41','r51'] \n",
    "style_layers = []\n",
    "style_weights = [sw*1e3/n**2 for sw,n in zip([sw1,sw2,sw3,sw4,sw5],[64,128,256,512,512])]\n",
    "# Content layers\n",
    "# content_layers = ['r12','r22','r32','r42','r52']\n",
    "# content_weights = [cw1*1e5,cw2*1e5,cw3*1e5,cw4*1e5,cw5*1e5]\n",
    "content_layers = ['r42']\n",
    "content_weights = [cw4*1e4]\n",
    "# Patch layers\n",
    "patch_layers = ['r31','r41']\n",
    "\n",
    "\n",
    "loss_layers = style_layers + content_layers + patch_layers\n",
    "loss_functions = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "loss_functions = [loss_fn.to(device) for loss_fn in loss_functions]\n",
    "weights = style_weights + content_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute optimization targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gram matrix targets\n",
    "#### Feature maps from style layers of the style images\n",
    "style1_fms_style = [A.detach() for A in vgg(style_image1, style_layers)]\n",
    "style2_fms_style = [A.detach() for A in vgg(style_image2, style_layers)]\n",
    "#### Difference between feature maps\n",
    "style_fms_style  = [style1_fms_style[i] - style2_fms_style[i] for i in range(len(style_layers))]\n",
    "#### Gram matrix of difference feature maps\n",
    "gramm_style = [GramMatrix()(A) for A in style_fms_style]\n",
    "#### Feature maps from style layers of the content image\n",
    "content_fms_style = [A.detach() for A in vgg(content_image, style_layers)]\n",
    "\n",
    "style_target = [GramMatrix()(style1_fms_style[i])-GramMatrix()(style2_fms_style[i]) for i in range(len(style_layers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Content targets\n",
    "\n",
    "#### Feature maps from content layers of the style images\n",
    "style1_fms_content = [A.detach() for A in vgg(style_image1, content_layers)]\n",
    "style2_fms_content = [A.detach() for A in vgg(style_image2, content_layers)]\n",
    "#### Difference between feature maps\n",
    "style_fms_content = [style1_fms_content[i] - style2_fms_content[i] for i in range(len(content_layers))]\n",
    "#### Feature maps from content layers of the content image\n",
    "content_fm_content = [A.detach() for A in vgg(content_image, content_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Patch targets\n",
    "\n",
    "#### Feature maps from patch layers of the style images\n",
    "style1_fms_patch = [A.detach() for A in vgg(style_image1, patch_layers)]\n",
    "style2_fms_patch = [A.detach() for A in vgg(style_image2, patch_layers)]\n",
    "#### Patches extracted from the feature maps\n",
    "style1_patches_lists, style1_weight_list = get_style_patch_weights(style1_fms_patch, device, k=patch_size)\n",
    "style2_patches_lists, style2_weight_list = get_style_patch_weights(style2_fms_patch, device, k=patch_size)\n",
    "#### Difference between corresponding patches\n",
    "diff_patches_list1, diff_patches_list2 = patch_difference(style1_patches_lists,style2_patches_lists)\n",
    "#### Different patches between style1 & style2\n",
    "style1_different_patches256 = []\n",
    "style2_different_patches256 = []\n",
    "for p1, p2 in zip(style1_patches_lists[0], style2_patches_lists[0]):\n",
    "    if not torch.eq(p1,p2).all():\n",
    "        style1_different_patches256.append(p1)\n",
    "        style2_different_patches256.append(p2)\n",
    "style1_different_patches512 = []\n",
    "style2_different_patches512 = []\n",
    "for p1, p2 in zip(style1_patches_lists[1], style2_patches_lists[1]):\n",
    "    if not torch.eq(p1,p2).all():\n",
    "        style1_different_patches512.append(p1)\n",
    "        style2_different_patches512.append(p2)\n",
    "style1_different_patches = [style1_different_patches256, style1_different_patches512]\n",
    "\n",
    "#### Feature maps from patch layers of the content image\n",
    "content_fms_patch = [A.detach() for A in vgg(content_image, patch_layers)]\n",
    "#### Patches extracted from the feature maps\n",
    "content_patches_lists, content_weight_list = get_style_patch_weights(style1_fms_patch, device, k=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights to use for convolution, in per channel size\n",
    "weight_list = [weight_maker(style_plist,patch_size,device) for style_plist in style1_different_patches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3090259378176., device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrf_loss_fn(content_fms_patch, style1_different_patches, weight_list, k=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_patches_lists = [content_patches_lists[0]+style1_different_patches256,content_patches_lists[1]+style1_different_patches512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights to use for convolution, in per channel size\n",
    "weight_list = [weight_maker(style_plist,patch_size,device) for style_plist in combined_patches_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2619749695488., device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrf_loss_fn(content_fms_patch, combined_patches_lists, weight_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run style transfer\n",
    "make_folders(output_path)\n",
    "\n",
    "max_iter = 600\n",
    "show_iter = 50\n",
    "optimizer = optim.LBFGS([opt_img])\n",
    "n_iter=[0]\n",
    "loss_list = []\n",
    "c_loss = []\n",
    "s_loss = []\n",
    "p_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while n_iter[0] <= max_iter:\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = vgg(opt_img, loss_layers)\n",
    "        content_layer_losses = []\n",
    "        style_layer_losses  = []\n",
    "        \n",
    "        # Divide between style feature maps, content feature maps and patch feature maps\n",
    "        opt_fms_style = out[:len(style_layers)]\n",
    "        opt_fms_content = out[len(style_layers):len(style_layers)+len(content_layers)]\n",
    "        opt_fms_patch = out[-len(patch_layers):]\n",
    "\n",
    "        # Difference between feature maps on style layers\n",
    "        diff_fms_style = [opt_fms_style[i] - content_fms_style[i] for i in range(len(style_layers))]\n",
    "        gramm_diff = [GramMatrix()(A) for A in diff_fms_style]\n",
    "        # Difference between gram matrix of feature map differences\n",
    "        style_layer_losses = [style_weights[i]*(nn.MSELoss()(gramm_diff[i], gramm_style[i])) for i in range(len(style_layers))]\n",
    "\n",
    "        ## Difference between feature maps on content layers\n",
    "        fms_diff = [opt_fms_content[i] - content_fm_content[i] for i in range(len(content_layers))]\n",
    "        content_layer_losses = [content_weights[i]*nn.MSELoss()(fms_diff[i],style_fms_content[i]) for i in range(len(content_layers))]\n",
    "\n",
    "\n",
    "        ## Differnce between the patches\n",
    "        #### Patches extracted from the opt feature maps\n",
    "        opt_patches_lists, opt_weight_list = get_style_patch_weights(opt_fms_patch, device, k=patch_size)\n",
    "        #### Difference between corresponding patches with content patches\n",
    "        diff_patches_list1_1, diff_patches_list2_2 = patch_difference(opt_patches_lists,content_patches_lists)\n",
    "\n",
    "        #### Patch loss\n",
    "#         patch_layer_losses = []\n",
    "#         for i in range(len(diff_patches_list1)):\n",
    "#             patch_layer_losses.append(nn.MSELoss()(diff_patches_list1_1[i],diff_patches_list1[i]))\n",
    "#         for i in range(len(diff_patches_list2)):\n",
    "#             patch_layer_losses.append(nn.MSELoss()(diff_patches_list2_2[i],diff_patches_list2[i]))\n",
    "        patch_layer_losses = mrf_loss_fn(opt_fms_patch, combined_patches_lists, weight_list, k=patch_size)\n",
    "\n",
    "\n",
    "        # losses\n",
    "        content_loss = sum(content_layer_losses)\n",
    "        style_loss   = sum(style_layer_losses)\n",
    "        patch_loss   = sum(patch_layer_losses)\n",
    "\n",
    "        loss = style_loss + content_loss + patch_loss\n",
    "        # layer_losses = content_layer_losses + style_layer_losses + patch_layer_losses\n",
    "\n",
    "        # # total loss\n",
    "        # loss = sum(layer_losses)\n",
    "\n",
    "        # for log\n",
    "        c_loss.append(content_loss)\n",
    "        s_loss.append(style_loss)\n",
    "        p_loss.append(patch_loss)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # backward calculation\n",
    "        loss.backward()\n",
    "\n",
    "        #print loss\n",
    "        if n_iter[0]%show_iter == 0:\n",
    "            print('Iteration: {}'.format(n_iter[0]))\n",
    "            if len(content_layers)>0: print('Content loss: {}'.format(content_loss.item()))\n",
    "            if len(style_layers)>0:   print('Style loss  : {}'.format(style_loss.item()))\n",
    "            if len(patch_layers)>0:   print('Patch loss  : {}'.format(patch_loss))\n",
    "            print('Total loss  : {}'.format(loss.item()))\n",
    "\n",
    "            # Save loss graph\n",
    "            # plt.plot(loss_list, label='Total loss')\n",
    "            if len(content_layers)>0:  plt.plot(c_loss, label='Content loss')\n",
    "            if len(style_layers)  >0:  plt.plot(s_loss, label='Style loss')\n",
    "            if len(patch_layers)  >0:  plt.plot(p_loss, label='Patch loss')\n",
    "            plt.legend()\n",
    "            plt.savefig(output_path + 'loss_graph.jpg')\n",
    "            plt.close()\n",
    "            # Save optimized image\n",
    "            out_img = postp(opt_img.data[0].cpu().squeeze(), image_size, result_invert)\n",
    "            out_img.save(output_path + 'outputs/{}.bmp'.format(n_iter[0]))\n",
    "        n_iter[0] += 1\n",
    "        return loss\n",
    "      \n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postp(opt_img.data[0].cpu().squeeze(), image_size, result_invert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sum images\n",
    "save_images(content_image.data[0].cpu().squeeze(), opt_img.data[0].cpu().squeeze(), style_image1.data[0].cpu().squeeze(), style_image2.data[0].cpu().squeeze(), image_size, output_path, n_iter, content_invert, style_invert, result_invert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
